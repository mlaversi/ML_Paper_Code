from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Initialisez la session Spark
spark = SparkSession.builder.appName("LaunderingAttemptAnalysis").getOrCreate()

# Définissez le schéma des données
schema = StructType([
    StructField("Timestamp", StringType(), True),
    StructField("ID1", StringType(), True),
    StructField("Account1", StringType(), True),
    StructField("ID2", StringType(), True),
    StructField("Account2", StringType(), True),
    StructField("Amount", DoubleType(), True),
    StructField("Currency1", StringType(), True),
    StructField("AmountReceived", DoubleType(), True),
    StructField("Currency2", StringType(), True),
    StructField("PaymentFormat", StringType(), True),
    StructField("Flag", StringType(), True),
])

# Remplacez "your_txt_file_path" par le chemin de votre fichier texte
txt_file_path = "your_txt_file_path"

# Chargez le fichier texte en tant que DataFrame en utilisant le schéma défini
txt_df = spark.read.csv(txt_file_path, schema=schema)

# Filtrer les lignes pertinentes
laundering_attempts_df = txt_df.filter(
    (F.col("Timestamp").startsWith("20")) &  # Assurez-vous que la ligne commence par une année (pour éviter les en-têtes)
    (F.col("Flag") == "1")  # Assurez-vous que la ligne a un indicateur de fraude (Flag = 1)
)

# Affichez le DataFrame résultant
laundering_attempts_df.show(truncate=False)

# Arrêtez la session Spark
spark.stop()
