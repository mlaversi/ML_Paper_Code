{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import scipy.io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "405919f8",
   "metadata": {},
   "source": [
    "### Image, Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "olympic-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare training set\n",
    "img_size = (28, 20)\n",
    "img_data = scipy.io.loadmat('frey_rawface.mat')[\"ff\"]\n",
    "img_data = img_data.T.reshape((-1, img_size[0], img_size[1]))\n",
    "trainX = torch.tensor(img_data[:int(0.8 * img_data.shape[0])], dtype=torch.float)/255."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38fb8082",
   "metadata": {},
   "source": [
    "### Funtions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(batch_size, device='cpu'):\n",
    "    indices = torch.randperm(trainX.shape[0])[:batch_size]\n",
    "    return trainX[indices].reshape(batch_size, -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorporate-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(batch_size, device='cpu'):\n",
    "    indices = torch.randperm(trainX.shape[0])[:batch_size]\n",
    "    return trainX[indices].reshape(batch_size, -1).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0cd3ddd",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "future-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, data_dim=2, context_dim=2, hidden_dim=200, constrain_mean=False):\n",
    "        super(Model, self).__init__()\n",
    "        '''\n",
    "        Model p(y|x) as N(mu, sigma) where mu and sigma are Neural Networks\n",
    "        '''\n",
    "\n",
    "        self.h = nn.Sequential(\n",
    "                 nn.Linear(context_dim, hidden_dim),\n",
    "                 nn.Tanh(),\n",
    "                 )\n",
    "        self.log_var = nn.Sequential(nn.Linear(hidden_dim, data_dim),)\n",
    "\n",
    "        if constrain_mean:\n",
    "            self.mu = nn.Sequential(nn.Linear(hidden_dim, data_dim), nn.Sigmoid())\n",
    "        else:\n",
    "            self.mu = nn.Sequential(nn.Linear(hidden_dim, data_dim), )\n",
    "\n",
    "    def get_mean_and_log_var(self, x):\n",
    "        h = self.h(x)\n",
    "        mu = self.mu(h)\n",
    "        log_var = self.log_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def forward(self, epsilon, x):\n",
    "        '''\n",
    "        Sample y ~ p(y|x) using the reparametrization trick\n",
    "        '''\n",
    "        mu, log_var = self.get_mean_and_log_var(x)\n",
    "        sigma = torch.sqrt(torch.exp(log_var))\n",
    "        return epsilon * sigma + mu\n",
    "\n",
    "    def compute_log_density(self, y, x):\n",
    "        '''\n",
    "        Compute log p(y|x)\n",
    "        '''\n",
    "        mu, log_var = self.get_mean_and_log_var(x)\n",
    "        log_density = -.5 * (torch.log(2 * torch.tensor(np.pi)) + log_var + (((y-mu)**2)/(torch.exp(log_var) + 1e-10))).sum(dim=1)\n",
    "        return log_density\n",
    "\n",
    "    def compute_KL(self, x):\n",
    "        '''\n",
    "        Assume that p(x) is a normal gaussian distribution; N(0, 1)\n",
    "        '''\n",
    "        mu, log_var = self.get_mean_and_log_var(x)\n",
    "        return -.5 * (1 + log_var - mu**2 - torch.exp(log_var)).sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "536e6e6d",
   "metadata": {},
   "source": [
    "### AutoEncoder !! 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infinite-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVEB(encoder, decoder, encoder_optimizer, decoder_optimizer, nb_epochs, M=100, L=1, latent_dim=2):\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(nb_epochs)):\n",
    "        x = get_minibatch(M, device=device)\n",
    "        epsilon = torch.normal(torch.zeros(M * L, latent_dim), torch.ones(latent_dim)).to(device)\n",
    "\n",
    "        # Compute the loss\n",
    "        z = encoder(epsilon, x)\n",
    "        log_likelihoods = decoder.compute_log_density(x, z)\n",
    "        kl_divergence = encoder.compute_KL(x)\n",
    "        loss = (kl_divergence - log_likelihoods.view(-1, L).mean(dim=1)).mean()\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebfd4b80",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "professional-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13043/1000000 [01:44<2:12:27, 124.18it/s]"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "encoder = Model(data_dim=2, context_dim=img_size[0]*img_size[1], hidden_dim=200).to(device)\n",
    "decoder = Model(data_dim=img_size[0]*img_size[1], context_dim=2, hidden_dim=200, constrain_mean=True).to(device)\n",
    "encoder_optimizer = torch.optim.Adagrad(encoder.parameters(), lr=0.01, weight_decay=0.5)\n",
    "decoder_optimizer = torch.optim.Adagrad(decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss = AVEB(encoder, decoder, encoder_optimizer, decoder_optimizer, 10**6)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(100*np.arange(len(loss)), -np.array(loss), c='r', label='AEVD (train)')\n",
    "plt.xscale('log')\n",
    "plt.xlim([10**5, 10**8])\n",
    "plt.ylim(0, 1600)\n",
    "plt.title(r'Frey Face, $N_z = 2$', fontsize=15)\n",
    "plt.ylabel(r'$\\mathcal{L}$', fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.savefig('Training_loss.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "grid_size = 10\n",
    "xx, yy = norm.ppf(np.meshgrid(np.linspace(0.1, .9, grid_size), np.linspace(0.1, .9, grid_size)))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 14), constrained_layout=False)\n",
    "grid = fig.add_gridspec(grid_size, grid_size, wspace=0, hspace=0)\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        img = decoder.get_mean_and_log_var(torch.tensor([[xx[i, j], yy[i, j]]], device=device, dtype=torch.float))\n",
    "        ax = fig.add_subplot(grid[i, j])\n",
    "        ax.imshow(np.clip(img[0].data.cpu().numpy().reshape(img_size[0], img_size[1]), 0, 1), cmap='gray', aspect='auto')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc64007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
